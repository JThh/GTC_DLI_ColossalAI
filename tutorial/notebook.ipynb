{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x_P2O8wv8EBn"
      },
      "source": [
        "## DLI Tutorial on ColossalAI Framework\n",
        "\n",
        "This tutorial serves as a gentle introduction into using ColossalAI for large-scale training and achieving multiple types of parallelism. There are in total 8 sections detailing the steps to realize:\n",
        "\n",
        "1. Environment setup; <br>\n",
        "2. Large-scale optimizer; <br>\n",
        "3. Hybrid parallelism; <br>\n",
        "4. Sequence parallelism; <br>\n",
        "5. Auto parallelism; <br> \n",
        "6. OPT model inference; <br>\n",
        "7. Fastfold inference; <br>\n",
        "8. Stable diffusion inference. <br>\n",
        "\n",
        "Run the codes below and you'll be amazed by the power and easy use of our ColossalAI. Accompany this notebook with the [video tutorial](https://drive.google.com/drive/folders/1FUu74wi6FTSfpNows-prvq3MFz53NlRf?usp=sharing) for more hands-on demonstrations. \n",
        "\n",
        "Also, our `README.md` in this folder also contains expected result screenshots which you may use to verify your outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGqQS7B88YWI"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "**[Caution]** this notebook can only be run with multiple GPUs due to parallelism settings. If you still want to run with one GPU, change config files to not use any parallelism explicitly.\n",
        "\n",
        "To run our examples smoothly, you need to have `conda` or `miniconda` installed on your device. Check out this [website](https://docs.conda.io/en/latest/miniconda.html) for more instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-85Gyad8Bo0",
        "outputId": "5260b8ff-0043-492e-9745-920a73005533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auto_parallel\n",
            "download_cifar10.py\n",
            "fastfold\n",
            "hybrid_parallel\n",
            "large_batch_optimizer\n",
            "opt\n",
            "README.md\n",
            "requirements.txt\n",
            "sequence_parallel\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'ColossalAI'...\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "ls ./tutorial/  # checkout list of examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7mwA40p-dOl",
        "outputId": "54aef497-aaa1-4f68-91e3-c8b3a149b844"
      },
      "outputs": [],
      "source": [
        "# Skip below if environment has been established via ../Dockerfile\n",
        "%%sh\n",
        "# conda create -n demo python=3.8\n",
        "# conda activate demo\n",
        "# pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# pip install colossalai==0.1.11rc3+torch1.12cu11.3 -f https://release.colossalai.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2G2Fjdb_8Xa",
        "outputId": "710bdb4c-cd3a-41b8-e769-ee97c8896638"
      },
      "outputs": [],
      "source": [
        "# Check versions of relevant packages\n",
        "!colossalai check -i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-9b4ciAAPaE"
      },
      "source": [
        "### Large-Scale Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-VM4iD3AV9u",
        "outputId": "1ef27f11-f7bc-4f02-871c-3ccba41ae395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/large_batch_optimizer\n"
          ]
        }
      ],
      "source": [
        "%cd ./tutorial/large_batch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39RptxqLBOQ1",
        "outputId": "4e62ec52-d86b-4cea-fd1b-69b032640575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.py  README.md  requirements.txt\ttest_ci.sh  train.py\n"
          ]
        }
      ],
      "source": [
        "# List of files in the ColossalAI/examples/tutorial/large_batch_optimizer directory\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLs0u7P_BmNP",
        "outputId": "9975d33d-f448-4d12-8cc6-1f149e3fb0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from colossalai.amp import AMP_TYPE\n",
            "\n",
            "# hyperparameters\n",
            "# BATCH_SIZE is as per GPU\n",
            "# global batch size = BATCH_SIZE x data parallel size\n",
            "BATCH_SIZE = 512\n",
            "LEARNING_RATE = 3e-3\n",
            "WEIGHT_DECAY = 0.3\n",
            "NUM_EPOCHS = 2\n",
            "WARMUP_EPOCHS = 1\n",
            "\n",
            "# model config\n",
            "NUM_CLASSES = 10\n",
            "\n",
            "fp16 = dict(mode=AMP_TYPE.NAIVE)\n",
            "clip_grad_norm = 1.0\n"
          ]
        }
      ],
      "source": [
        "!cat config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI-V0ozQDE4b",
        "outputId": "a2d63669-57b5-42db-a169-dc7fb2aabd21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import torch\n",
            "import torch.nn as nn\n",
            "from torchvision.models import resnet18\n",
            "from tqdm import tqdm\n",
            "\n",
            "import colossalai\n",
            "from colossalai.core import global_context as gpc\n",
            "from colossalai.logging import get_dist_logger\n",
            "from colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\n",
            "from colossalai.nn.optimizer import Lamb, Lars\n",
            "\n",
            "\n",
            "class DummyDataloader():\n",
            "\n",
            "    def __init__(self, length, batch_size):\n",
            "        self.length = length\n",
            "        self.batch_size = batch_size\n",
            "\n",
            "    def generate(self):\n",
            "        data = torch.rand(self.batch_size, 3, 224, 224)\n",
            "        label = torch.randint(low=0, high=10, size=(self.batch_size,))\n",
            "        return data, label\n",
            "\n",
            "    def __iter__(self):\n",
            "        self.step = 0\n",
            "        return self\n",
            "\n",
            "    def __next__(self):\n",
            "        if self.step < self.length:\n",
            "            self.step += 1\n",
            "            return self.generate()\n",
            "        else:\n",
            "            raise StopIteration\n",
            "\n",
            "    def __len__(self):\n",
            "        return self.length\n",
            "\n",
            "\n",
            "def main():\n",
            "    # initialize distributed setting\n",
            "    parser = colossalai.get_default_parser()\n",
            "    parser.add_argument('--optimizer',\n",
            "                        choices=['lars', 'lamb'],\n",
            "                        help=\"Choose your large-batch optimizer\",\n",
            "                        required=True)\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # launch from torch\n",
            "    colossalai.launch_from_torch(config=args.config)\n",
            "\n",
            "    # get logger\n",
            "    logger = get_dist_logger()\n",
            "    logger.info(\"initialized distributed environment\", ranks=[0])\n",
            "\n",
            "    # create synthetic dataloaders\n",
            "    train_dataloader = DummyDataloader(length=10, batch_size=gpc.config.BATCH_SIZE)\n",
            "    test_dataloader = DummyDataloader(length=5, batch_size=gpc.config.BATCH_SIZE)\n",
            "\n",
            "    # build model\n",
            "    model = resnet18(num_classes=gpc.config.NUM_CLASSES)\n",
            "\n",
            "    # create loss function\n",
            "    criterion = nn.CrossEntropyLoss()\n",
            "\n",
            "    # create optimizer\n",
            "    if args.optimizer == \"lars\":\n",
            "        optim_cls = Lars\n",
            "    elif args.optimizer == \"lamb\":\n",
            "        optim_cls = Lamb\n",
            "    optimizer = optim_cls(model.parameters(), lr=gpc.config.LEARNING_RATE, weight_decay=gpc.config.WEIGHT_DECAY)\n",
            "\n",
            "    # create lr scheduler\n",
            "    lr_scheduler = CosineAnnealingWarmupLR(optimizer=optimizer,\n",
            "                                           total_steps=gpc.config.NUM_EPOCHS,\n",
            "                                           warmup_steps=gpc.config.WARMUP_EPOCHS)\n",
            "\n",
            "    # initialize\n",
            "    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model=model,\n",
            "                                                                         optimizer=optimizer,\n",
            "                                                                         criterion=criterion,\n",
            "                                                                         train_dataloader=train_dataloader,\n",
            "                                                                         test_dataloader=test_dataloader)\n",
            "\n",
            "    logger.info(\"Engine is built\", ranks=[0])\n",
            "\n",
            "    for epoch in range(gpc.config.NUM_EPOCHS):\n",
            "        # training\n",
            "        engine.train()\n",
            "        data_iter = iter(train_dataloader)\n",
            "\n",
            "        if gpc.get_global_rank() == 0:\n",
            "            description = 'Epoch {} / {}'.format(epoch, gpc.config.NUM_EPOCHS)\n",
            "            progress = tqdm(range(len(train_dataloader)), desc=description)\n",
            "        else:\n",
            "            progress = range(len(train_dataloader))\n",
            "        for _ in progress:\n",
            "            engine.zero_grad()\n",
            "            engine.execute_schedule(data_iter, return_output_label=False)\n",
            "            engine.step()\n",
            "            lr_scheduler.step()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n"
          ]
        }
      ],
      "source": [
        "!cat train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3-qD5u-B-jf"
      },
      "outputs": [],
      "source": [
        "# Install additional requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAKgExC5CI4x",
        "outputId": "6a00635d-1c07-44f3-f3aa-bcf310bb44b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
            "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
            "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
            "  dispatch key: Meta\n",
            "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
            "       new kernel: registered at /dev/null:219 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
            "  self.m.impl(name, dispatch_key, fn)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
            "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
            "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
            "  dispatch key: Meta\n",
            "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
            "       new kernel: registered at /dev/null:219 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
            "  self.m.impl(name, dispatch_key, fn)\n",
            "[02/02/23 12:17:46] INFO     colossalai - colossalai - INFO:                    \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/c\n",
            "                             ontext/parallel_context.py:521 set_device          \n",
            "                    INFO     colossalai - colossalai - INFO: process rank 0 is  \n",
            "                             bound to device 0                                  \n",
            "[02/02/23 12:17:47] INFO     colossalai - colossalai - INFO:                    \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/c\n",
            "                             ontext/parallel_context.py:557 set_seed            \n",
            "                    INFO     colossalai - colossalai - INFO: initialized seed on\n",
            "                             rank 0, numpy: 1024, python random: 1024,          \n",
            "                             ParallelMode.DATA: 1024, ParallelMode.TENSOR:      \n",
            "                             1024,the default parallel seed is                  \n",
            "                             ParallelMode.DATA.                                 \n",
            "                    INFO     colossalai - colossalai - INFO:                    \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/i\n",
            "                             nitialize.py:117 launch                            \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed        \n",
            "                             environment is initialized, data parallel size: 1, \n",
            "                             pipeline parallel size: 1, tensor parallel size: 1 \n",
            "                    INFO     colossalai - colossalai - INFO: train.py:53 main   \n",
            "                    INFO     colossalai - colossalai - INFO: initialized        \n",
            "                             distributed environment                            \n",
            "                    INFO     colossalai - colossalai - INFO:                    \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/i\n",
            "                             nitialize.py:266 initialize                        \n",
            "                    INFO     colossalai - colossalai - INFO:                    \n",
            "                             ========== Your Config ========                    \n",
            "                             {'BATCH_SIZE': 512,                                \n",
            "                              'LEARNING_RATE': 0.003,                           \n",
            "                              'NUM_CLASSES': 10,                                \n",
            "                              'NUM_EPOCHS': 2,                                  \n",
            "                              'WARMUP_EPOCHS': 1,                               \n",
            "                              'WEIGHT_DECAY': 0.3,                              \n",
            "                              'clip_grad_norm': 1.0,                            \n",
            "                              'fp16': {'mode': <AMP_TYPE.NAIVE: 'naive'>}}      \n",
            "                             ================================                   \n",
            "                                                                                \n",
            "                    INFO     colossalai - colossalai - INFO:                    \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/i\n",
            "                             nitialize.py:278 initialize                        \n",
            "[02/02/23 12:17:48] INFO     colossalai - colossalai - INFO: cuDNN benchmark =  \n",
            "                             False, deterministic = False                       \n",
            "                    WARNING  colossalai - colossalai - WARNING:                 \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/i\n",
            "                             nitialize.py:304 initialize                        \n",
            "                    WARNING  colossalai - colossalai - WARNING: Initializing an \n",
            "                             non ZeRO model with optimizer class                \n",
            "                    WARNING  colossalai - colossalai - WARNING:                 \n",
            "                             /usr/local/lib/python3.8/dist-packages/colossalai/i\n",
            "                             nitialize.py:443 initialize                        \n",
            "                    WARNING  colossalai - colossalai - WARNING: No PyTorch DDP  \n",
            "                             or gradient handler is set up, please make sure you\n",
            "                             do not need to all-reduce the gradients after a    \n",
            "                             training step.                                     \n",
            "                    INFO     colossalai - colossalai - INFO: train.py:84 main   \n",
            "                    INFO     colossalai - colossalai - INFO: Engine is built    \n",
            "Epoch 0 / 2:   0%|          | 0/10 [00:00<?, ?it/s]=========================================================================================\n",
            "No pre-built kernel is found, build and load the fused_optim kernel during runtime now\n",
            "=========================================================================================\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/colossalai/torch_extensions/torch1.13_cu11.6/build.ninja...\n",
            "Building extension module fused_optim...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/7] c++ -MMD -MF colossal_C_frontend.o.d -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/colossal_C_frontend.cpp -o colossal_C_frontend.o \n",
            "[2/7] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -std=c++14 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/multi_tensor_sgd_kernel.cu -o multi_tensor_sgd_kernel.cuda.o \n",
            "[3/7] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -std=c++14 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/multi_tensor_scale_kernel.cu -o multi_tensor_scale_kernel.cuda.o \n",
            "[4/7] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -std=c++14 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "[5/7] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -std=c++14 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/multi_tensor_l2norm_kernel.cu -o multi_tensor_l2norm_kernel.cuda.o \n",
            "[6/7] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_optim -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/kernels/include -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -std=c++14 -c /usr/local/lib/python3.8/dist-packages/colossalai/kernel/cuda_native/csrc/multi_tensor_lamb.cu -o multi_tensor_lamb.cuda.o \n",
            "[7/7] c++ colossal_C_frontend.o multi_tensor_sgd_kernel.cuda.o multi_tensor_scale_kernel.cuda.o multi_tensor_adam.cuda.o multi_tensor_l2norm_kernel.cuda.o multi_tensor_lamb.cuda.o -shared -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_optim.so\n",
            "Loading extension module fused_optim...\n",
            "Time to load fused_optim op: 223.8772633075714 seconds\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch 0 / 2: 100%|██████████| 10/10 [04:03<00:00, 24.37s/it]\n",
            "Epoch 1 / 2: 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
          ]
        }
      ],
      "source": [
        "# Example trial (20 steps) with lars optimizer\n",
        "!colossalai run --nproc_per_node 4 train.py --config config.py --optimizer lars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5fK72FRCigM"
      },
      "outputs": [],
      "source": [
        "# Now use Lamb optimizer\n",
        "!colossalai run --nproc_per_node 4 train.py --config config.py --optimizer lamb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GRh8gA0BXXS",
        "outputId": "c7aadf1f-4968-4650-f331-0a244306070a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Return to parent directory\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrFehJ8lEHqe"
      },
      "source": [
        "### Hybrid Parallelism\n",
        "\n",
        "Example of hybriding pipeline and 1-D (2-D) tensor parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLTXVsVjENKE",
        "outputId": "58ea5654-2679-4221-b894-19703cb2adb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/hybrid_parallel\n"
          ]
        }
      ],
      "source": [
        "%cd ./tutorial/hybrid_parallel/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-7VfJyLJeqe"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7IO37XkEfd4",
        "outputId": "b1c5bf5d-bd37-4cdc-8668-d823b5c40ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from colossalai.amp import AMP_TYPE\n",
            "\n",
            "# hyperparameters\n",
            "# BATCH_SIZE is as per GPU\n",
            "# global batch size = BATCH_SIZE x data parallel size\n",
            "BATCH_SIZE = 4\n",
            "LEARNING_RATE = 3e-3\n",
            "WEIGHT_DECAY = 0.3\n",
            "NUM_EPOCHS = 2\n",
            "WARMUP_EPOCHS = 1\n",
            "\n",
            "# model config\n",
            "IMG_SIZE = 224\n",
            "PATCH_SIZE = 16\n",
            "HIDDEN_SIZE = 128\n",
            "DEPTH = 4\n",
            "NUM_HEADS = 4\n",
            "MLP_RATIO = 2\n",
            "NUM_CLASSES = 10\n",
            "CHECKPOINT = False\n",
            "SEQ_LENGTH = (IMG_SIZE // PATCH_SIZE)**2 + 1    # add 1 for cls token\n",
            "\n",
            "# parallel setting\n",
            "TENSOR_PARALLEL_SIZE = 2\n",
            "TENSOR_PARALLEL_MODE = '1d'\n",
            "\n",
            "parallel = dict(\n",
            "    pipeline=2,\n",
            "    tensor=dict(mode=TENSOR_PARALLEL_MODE, size=TENSOR_PARALLEL_SIZE),\n",
            ")\n",
            "\n",
            "fp16 = dict(mode=AMP_TYPE.NAIVE)\n",
            "clip_grad_norm = 1.0\n",
            "\n",
            "# pipeline config\n",
            "NUM_MICRO_BATCHES = parallel['pipeline']\n"
          ]
        }
      ],
      "source": [
        "!cat config.py  # inspect configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKybBJYhEivr",
        "outputId": "f2296517-6fad-4d9e-8288-32093fd01d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import os\n",
            "\n",
            "import torch\n",
            "from titans.model.vit.vit import _create_vit_model\n",
            "from tqdm import tqdm\n",
            "\n",
            "import colossalai\n",
            "from colossalai.context import ParallelMode\n",
            "from colossalai.core import global_context as gpc\n",
            "from colossalai.logging import get_dist_logger\n",
            "from colossalai.nn import CrossEntropyLoss\n",
            "from colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\n",
            "from colossalai.pipeline.pipelinable import PipelinableContext\n",
            "from colossalai.utils import is_using_pp\n",
            "\n",
            "\n",
            "class DummyDataloader():\n",
            "\n",
            "    def __init__(self, length, batch_size):\n",
            "        self.length = length\n",
            "        self.batch_size = batch_size\n",
            "\n",
            "    def generate(self):\n",
            "        data = torch.rand(self.batch_size, 3, 224, 224)\n",
            "        label = torch.randint(low=0, high=10, size=(self.batch_size,))\n",
            "        return data, label\n",
            "\n",
            "    def __iter__(self):\n",
            "        self.step = 0\n",
            "        return self\n",
            "\n",
            "    def __next__(self):\n",
            "        if self.step < self.length:\n",
            "            self.step += 1\n",
            "            return self.generate()\n",
            "        else:\n",
            "            raise StopIteration\n",
            "\n",
            "    def __len__(self):\n",
            "        return self.length\n",
            "\n",
            "\n",
            "def main():\n",
            "    # launch from torch\n",
            "    parser = colossalai.get_default_parser()\n",
            "    args = parser.parse_args()\n",
            "    colossalai.launch_from_torch(config=args.config)\n",
            "\n",
            "    # get logger\n",
            "    logger = get_dist_logger()\n",
            "    logger.info(\"initialized distributed environment\", ranks=[0])\n",
            "\n",
            "    if hasattr(gpc.config, 'LOG_PATH'):\n",
            "        if gpc.get_global_rank() == 0:\n",
            "            log_path = gpc.config.LOG_PATH\n",
            "            if not os.path.exists(log_path):\n",
            "                os.mkdir(log_path)\n",
            "            logger.log_to_file(log_path)\n",
            "\n",
            "    use_pipeline = is_using_pp()\n",
            "\n",
            "    # create model\n",
            "    model_kwargs = dict(img_size=gpc.config.IMG_SIZE,\n",
            "                        patch_size=gpc.config.PATCH_SIZE,\n",
            "                        hidden_size=gpc.config.HIDDEN_SIZE,\n",
            "                        depth=gpc.config.DEPTH,\n",
            "                        num_heads=gpc.config.NUM_HEADS,\n",
            "                        mlp_ratio=gpc.config.MLP_RATIO,\n",
            "                        num_classes=10,\n",
            "                        init_method='jax',\n",
            "                        checkpoint=gpc.config.CHECKPOINT)\n",
            "\n",
            "    if use_pipeline:\n",
            "        pipelinable = PipelinableContext()\n",
            "        with pipelinable:\n",
            "            model = _create_vit_model(**model_kwargs)\n",
            "        pipelinable.to_layer_list()\n",
            "        pipelinable.policy = \"uniform\"\n",
            "        model = pipelinable.partition(1, gpc.pipeline_parallel_size, gpc.get_local_rank(ParallelMode.PIPELINE))\n",
            "    else:\n",
            "        model = _create_vit_model(**model_kwargs)\n",
            "\n",
            "    # count number of parameters\n",
            "    total_numel = 0\n",
            "    for p in model.parameters():\n",
            "        total_numel += p.numel()\n",
            "    if not gpc.is_initialized(ParallelMode.PIPELINE):\n",
            "        pipeline_stage = 0\n",
            "    else:\n",
            "        pipeline_stage = gpc.get_local_rank(ParallelMode.PIPELINE)\n",
            "    logger.info(f\"number of parameters: {total_numel} on pipeline stage {pipeline_stage}\")\n",
            "\n",
            "    # use synthetic dataset\n",
            "    # we train for 10 steps and eval for 5 steps per epoch\n",
            "    train_dataloader = DummyDataloader(length=10, batch_size=gpc.config.BATCH_SIZE)\n",
            "    test_dataloader = DummyDataloader(length=5, batch_size=gpc.config.BATCH_SIZE)\n",
            "\n",
            "    # create loss function\n",
            "    criterion = CrossEntropyLoss(label_smoothing=0.1)\n",
            "\n",
            "    # create optimizer\n",
            "    optimizer = torch.optim.AdamW(model.parameters(), lr=gpc.config.LEARNING_RATE, weight_decay=gpc.config.WEIGHT_DECAY)\n",
            "\n",
            "    # create lr scheduler\n",
            "    lr_scheduler = CosineAnnealingWarmupLR(optimizer=optimizer,\n",
            "                                           total_steps=gpc.config.NUM_EPOCHS,\n",
            "                                           warmup_steps=gpc.config.WARMUP_EPOCHS)\n",
            "\n",
            "    # initialize\n",
            "    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model=model,\n",
            "                                                                         optimizer=optimizer,\n",
            "                                                                         criterion=criterion,\n",
            "                                                                         train_dataloader=train_dataloader,\n",
            "                                                                         test_dataloader=test_dataloader)\n",
            "\n",
            "    logger.info(\"Engine is built\", ranks=[0])\n",
            "\n",
            "    for epoch in range(gpc.config.NUM_EPOCHS):\n",
            "        # training\n",
            "        engine.train()\n",
            "        data_iter = iter(train_dataloader)\n",
            "\n",
            "        if gpc.get_global_rank() == 0:\n",
            "            description = 'Epoch {} / {}'.format(epoch, gpc.config.NUM_EPOCHS)\n",
            "            progress = tqdm(range(len(train_dataloader)), desc=description)\n",
            "        else:\n",
            "            progress = range(len(train_dataloader))\n",
            "        for _ in progress:\n",
            "            engine.zero_grad()\n",
            "            engine.execute_schedule(data_iter, return_output_label=False)\n",
            "            engine.step()\n",
            "            lr_scheduler.step()\n",
            "    gpc.destroy()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n"
          ]
        }
      ],
      "source": [
        "!cat train.py  # inspect training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOs1FbvvEsGP"
      },
      "outputs": [],
      "source": [
        "# Execute example trial\n",
        "!colossalai run --nproc_per_node 4 train.py --config config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRgTRMX0FiP0"
      },
      "outputs": [],
      "source": [
        "# Let's now tweak the configuation to adopt 2-D tensor parallelism\n",
        "with open('config.py', 'r') as file:\n",
        "  data = file.readlines()\n",
        "\n",
        "# Change line 24-25\n",
        "data[23] = 'TENSOR_PARALLEL_SIZE = 4\\n'\n",
        "data[24] = \"TENSOR_PARALLEL_MODE = '2d'\\n\"\n",
        "\n",
        "with open('config.py', 'w') as file:\n",
        "  file.writelines(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s-rxf_SHBrj",
        "outputId": "ff721f05-c1a9-403a-b579-240717dcc889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.py  README.md  requirements.txt\ttest_ci.sh  train.py\n"
          ]
        }
      ],
      "source": [
        "# New trial with hybrid of 2-D tensor parallism with pipeline parallelism \n",
        "!colossalai run --nproc_per_node 8 train.py --config config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUGD6xqVEbZf",
        "outputId": "fe72e3df-8614-49cc-f332-90a47b9e38be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Return to parent directory\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1qbh54oHbNf"
      },
      "source": [
        "### Sequence Parallelism\n",
        "\n",
        "Interested users may refer to [this paper](https://arxiv.org/abs/2105.13120) for implementation details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRmQTlZnHa3V",
        "outputId": "35cca02d-9f24-4b5d-efc7-2c057b153ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/sequence_parallel\n"
          ]
        }
      ],
      "source": [
        "%cd ./tutorial/sequence_parallel/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ZwjFrfIVOG",
        "outputId": "0f837a3d-efa8-4971-97c9-09aa02d58be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.py  loss_func\t model\t    requirements.txt  train.py\n",
            "data\t   lr_scheduler  README.md  test_ci.sh\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tdAHcPeJBMt"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7QATeRtH2Ju",
        "outputId": "393ea015-1b2e-4db9-cdce-0ae395746184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from colossalai.amp import AMP_TYPE\n",
            "\n",
            "# hyper-parameters\n",
            "TRAIN_ITERS = 10\n",
            "DECAY_ITERS = 4\n",
            "WARMUP_FRACTION = 0.01\n",
            "GLOBAL_BATCH_SIZE = 32    # dp world size * sentences per GPU\n",
            "EVAL_ITERS = 10\n",
            "EVAL_INTERVAL = 10\n",
            "LR = 0.0001\n",
            "MIN_LR = 1e-05\n",
            "WEIGHT_DECAY = 0.01\n",
            "SEQ_LENGTH = 128\n",
            "\n",
            "# BERT config\n",
            "DEPTH = 4\n",
            "NUM_ATTENTION_HEADS = 4\n",
            "HIDDEN_SIZE = 128\n",
            "\n",
            "# model config\n",
            "ADD_BINARY_HEAD = False\n",
            "\n",
            "# random seed\n",
            "SEED = 1234\n",
            "\n",
            "# pipeline config\n",
            "# only enabled when pipeline > 1\n",
            "NUM_MICRO_BATCHES = 4\n",
            "\n",
            "# colossalai config\n",
            "parallel = dict(pipeline=1, tensor=dict(size=2, mode='sequence'))\n",
            "\n",
            "fp16 = dict(mode=AMP_TYPE.NAIVE, verbose=True)\n",
            "\n",
            "gradient_handler = [dict(type='SequenceParallelGradientHandler')]\n"
          ]
        }
      ],
      "source": [
        "# Sequence parallelism is sized 2.\n",
        "!cat config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-X01lwAH4nB",
        "outputId": "ff5ee43f-b342-423d-8e6b-d7854aedd8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import argparse\n",
            "\n",
            "import torch\n",
            "from data.bert_helper import SequenceParallelDataIterator, get_batch_for_sequence_parallel\n",
            "from data.dummy_dataloader import DummyDataloader\n",
            "from loss_func.bert_loss import BertLoss\n",
            "from lr_scheduler import AnnealingLR\n",
            "from model.bert import BertForPretrain, build_pipeline_bert\n",
            "\n",
            "import colossalai\n",
            "from colossalai.amp import AMP_TYPE\n",
            "from colossalai.context.parallel_mode import ParallelMode\n",
            "from colossalai.core import global_context as gpc\n",
            "from colossalai.engine.schedule import PipelineSchedule\n",
            "from colossalai.kernel import LayerNorm\n",
            "from colossalai.logging import get_dist_logger\n",
            "from colossalai.nn.optimizer import FusedAdam\n",
            "from colossalai.utils import MultiTimer, is_using_pp\n",
            "\n",
            "\n",
            "def process_batch_data(batch_data):\n",
            "    tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = batch_data\n",
            "    if gpc.is_first_rank(ParallelMode.PIPELINE):\n",
            "        data = dict(input_ids=tokens, attention_masks=padding_mask, tokentype_ids=types, lm_labels=lm_labels)\n",
            "    else:\n",
            "        data = dict(attention_masks=padding_mask, tokentype_ids=types, lm_labels=lm_labels)\n",
            "    label = dict(loss_mask=loss_mask, sentence_order=sentence_order)\n",
            "    return data, label\n",
            "\n",
            "\n",
            "def parse_args():\n",
            "    parser = argparse.ArgumentParser()\n",
            "    parser.add_argument('-s', '--synthetic', action=\"store_true\", help=\"whether use synthetic data\")\n",
            "    return parser.parse_args()\n",
            "\n",
            "\n",
            "def pipeline_data_process_func(stage_output, micro_batch_data):\n",
            "    tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = micro_batch_data\n",
            "    if gpc.is_first_rank(ParallelMode.PIPELINE):\n",
            "        data = (tokens, padding_mask, types, lm_labels)\n",
            "        label = (loss_mask, sentence_order)\n",
            "    else:\n",
            "        data = (stage_output, padding_mask, types, lm_labels)\n",
            "        label = (loss_mask, sentence_order)\n",
            "    return data, label\n",
            "\n",
            "\n",
            "def main():\n",
            "    # initialize\n",
            "    args = parse_args()\n",
            "    colossalai.launch_from_torch(config='./config.py', seed=1234, backend='nccl')\n",
            "\n",
            "    logger = get_dist_logger()\n",
            "\n",
            "    # build synthetic dataloader\n",
            "    BATCH_SIZE_PER_GPUS = gpc.config.GLOBAL_BATCH_SIZE // gpc.get_world_size(ParallelMode.DATA)\n",
            "    VOCAB_SIZE = 30528\n",
            "    trainloader = DummyDataloader(batch_size=BATCH_SIZE_PER_GPUS,\n",
            "                                  vocab_size=VOCAB_SIZE,\n",
            "                                  seq_length=gpc.config.SEQ_LENGTH)\n",
            "    validloader = DummyDataloader(batch_size=BATCH_SIZE_PER_GPUS,\n",
            "                                  vocab_size=VOCAB_SIZE,\n",
            "                                  seq_length=gpc.config.SEQ_LENGTH)\n",
            "\n",
            "    logger.info(\"Dataloaders are built\", ranks=[0])\n",
            "\n",
            "    # build model\n",
            "    if hasattr(gpc.config, 'fp16') and gpc.config.fp16.get('mode') == AMP_TYPE.NAIVE:\n",
            "        is_naive_fp16 = True\n",
            "    else:\n",
            "        is_naive_fp16 = False\n",
            "\n",
            "    use_pipeline = is_using_pp()\n",
            "    kwargs = dict(vocab_size=VOCAB_SIZE,\n",
            "                  hidden_size=gpc.config.HIDDEN_SIZE,\n",
            "                  max_sequence_length=gpc.config.SEQ_LENGTH,\n",
            "                  num_attention_heads=gpc.config.NUM_ATTENTION_HEADS,\n",
            "                  convert_fp16_to_fp32_in_softmax=True,\n",
            "                  is_naive_fp16=is_naive_fp16,\n",
            "                  add_binary_head=gpc.config.ADD_BINARY_HEAD)\n",
            "\n",
            "    if use_pipeline:\n",
            "        model = build_pipeline_bert(num_layers=gpc.config.DEPTH, num_chunks=1, **kwargs)\n",
            "    else:\n",
            "        model = BertForPretrain(num_layers=gpc.config.DEPTH, **kwargs)\n",
            "\n",
            "    model = model.half()\n",
            "    model.reset_parameters()\n",
            "    logger.info(f\"Model is built with softmax in fp32 = {is_naive_fp16}\", ranks=[0])\n",
            "\n",
            "    total_numel = 0\n",
            "    for p in model.parameters():\n",
            "        total_numel += p.numel()\n",
            "    logger.info(f\"This model has {total_numel} parameters\")\n",
            "\n",
            "    # build criterion\n",
            "    criterion = BertLoss()\n",
            "    logger.info(\"Criterion is built\", ranks=[0])\n",
            "\n",
            "    # layernorm and bias has no weight decay\n",
            "    weight_decay_params = {'params': []}\n",
            "    no_weight_decay_params = {'params': [], 'weight_decay': 0.0}\n",
            "    for module_ in model.modules():\n",
            "        if isinstance(module_, LayerNorm):\n",
            "            no_weight_decay_params['params'].extend([p for p in list(module_._parameters.values()) if p is not None])\n",
            "        else:\n",
            "            weight_decay_params['params'].extend(\n",
            "                [p for n, p in list(module_._parameters.items()) if p is not None and n != 'bias'])\n",
            "            no_weight_decay_params['params'].extend(\n",
            "                [p for n, p in list(module_._parameters.items()) if p is not None and n == 'bias'])\n",
            "\n",
            "    logger.info(\n",
            "        f\"without weight decay param: {len(no_weight_decay_params['params'])}, with weight decay param: {len(weight_decay_params['params'])}\"\n",
            "    )\n",
            "    # optimizer\n",
            "    optimizer = FusedAdam((weight_decay_params, no_weight_decay_params),\n",
            "                          lr=gpc.config.LR,\n",
            "                          weight_decay=gpc.config.WEIGHT_DECAY)\n",
            "    logger.info(\"Optimizer is built\", ranks=[0])\n",
            "\n",
            "    # lr scheduler\n",
            "    # follow Megatron-LM setting\n",
            "    warmup_steps = int(gpc.config.DECAY_ITERS * gpc.config.WARMUP_FRACTION)\n",
            "    lr_scheduler = AnnealingLR(optimizer=optimizer,\n",
            "                               max_lr=gpc.config.LR,\n",
            "                               min_lr=gpc.config.MIN_LR,\n",
            "                               warmup_steps=warmup_steps,\n",
            "                               decay_steps=gpc.config.DECAY_ITERS,\n",
            "                               decay_style='linear')\n",
            "    logger.info(f\"LR Scheduler is built with {warmup_steps} warmup steps and {gpc.config.DECAY_ITERS} decay steps\")\n",
            "\n",
            "    # # init\n",
            "    engine, *dummy = colossalai.initialize(model, optimizer, criterion, verbose=True)\n",
            "\n",
            "    # build timer\n",
            "    timer = MultiTimer()\n",
            "    skip_iters = 0\n",
            "\n",
            "    # build loss tracker\n",
            "    accumulated_train_loss = torch.zeros(1, dtype=torch.float32).cuda()\n",
            "    accumulated_eval_loss = torch.zeros(1, dtype=torch.float32).cuda()\n",
            "\n",
            "    # build data iters for pipeline parallel\n",
            "    if use_pipeline:\n",
            "        train_data_iter = SequenceParallelDataIterator(trainloader)\n",
            "        valid_data_iter = SequenceParallelDataIterator(validloader)\n",
            "        engine.schedule.data_process_func = pipeline_data_process_func\n",
            "\n",
            "    logger.info(\"start training\")\n",
            "\n",
            "    for step in range(1, gpc.config.TRAIN_ITERS + 1):\n",
            "        timer.start('train-iterations')\n",
            "        engine.train()\n",
            "        if use_pipeline:\n",
            "            engine.zero_grad()\n",
            "            _, _, train_loss = engine.execute_schedule(train_data_iter, return_output_label=False)\n",
            "            engine.step()\n",
            "        else:\n",
            "            tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = get_batch_for_sequence_parallel(\n",
            "                trainloader)\n",
            "            engine.zero_grad()\n",
            "            lm_loss, sop_output = engine(tokens, padding_mask, types, lm_labels)\n",
            "            train_loss = engine.criterion(lm_loss, sop_output, loss_mask, sentence_order)\n",
            "            engine.backward(train_loss)\n",
            "            engine.step()\n",
            "        timer.stop('train-iterations', keep_in_history=True)\n",
            "\n",
            "        if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
            "            accumulated_train_loss += train_loss\n",
            "\n",
            "        lr_scheduler.step()\n",
            "\n",
            "        if step % gpc.config.EVAL_INTERVAL == 0:\n",
            "            engine.eval()\n",
            "\n",
            "            for j in range(gpc.config.EVAL_ITERS):\n",
            "                with torch.no_grad():\n",
            "                    if use_pipeline:\n",
            "                        _, _, eval_loss = engine.execute_schedule(valid_data_iter,\n",
            "                                                                  forward_only=True,\n",
            "                                                                  return_output_label=False)\n",
            "                    else:\n",
            "                        tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = get_batch_for_sequence_parallel(\n",
            "                            validloader)\n",
            "                        lm_loss, sop_output = engine(tokens, padding_mask, types, lm_labels)\n",
            "                        eval_loss = engine.criterion(lm_loss, sop_output, loss_mask, sentence_order)\n",
            "\n",
            "                    if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
            "                        accumulated_eval_loss += eval_loss\n",
            "\n",
            "            if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
            "                accumulated_eval_loss /= gpc.config.EVAL_ITERS\n",
            "                accumulated_train_loss /= gpc.config.EVAL_INTERVAL\n",
            "\n",
            "            timer_string = []\n",
            "            for n, t in timer:\n",
            "                timer_string.append(f\"{n}: {t.get_history_mean()*1000:.5f}\")\n",
            "            timer_string = ' | '.join(timer_string)\n",
            "            lr = list(engine.optimizer.param_groups)[0]['lr']\n",
            "            loss_scale = engine.optimizer.optim.loss_scale.item()\n",
            "\n",
            "            if gpc.is_initialized(ParallelMode.PIPELINE):\n",
            "                ranks = [gpc.get_ranks_in_group(ParallelMode.PIPELINE)[-1]]\n",
            "            else:\n",
            "                ranks = [0]\n",
            "            logger.info(f'Step {step} / {gpc.config.TRAIN_ITERS} | Train Loss: {accumulated_train_loss.item():.5g} ' +\n",
            "                        f'| Eval Loss: {accumulated_eval_loss.item():.5g} ' + f'| Loss Scale: {loss_scale}' +\n",
            "                        f\"| Learning rate: {lr} | \" + timer_string,\n",
            "                        ranks=ranks)\n",
            "\n",
            "            for n, t in timer:\n",
            "                t.reset()\n",
            "            accumulated_eval_loss.zero_()\n",
            "            accumulated_train_loss.zero_()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n"
          ]
        }
      ],
      "source": [
        "!cat train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quhl6RAhH9p3"
      },
      "outputs": [],
      "source": [
        "# Vanilla trial: sequence parallelism (2) \n",
        "!colossalai run --nproc_per_node 2 train.py -s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqRe_ZcXIxFB"
      },
      "outputs": [],
      "source": [
        "# Let's now tweak the configuation to adopt size=2 pipeline parallelism\n",
        "with open('config.py', 'r') as file:\n",
        "  data = file.readlines()\n",
        "\n",
        "# Change line 31\n",
        "data[30] = \"parallel = dict(pipeline=2, tensor=dict(size=2, mode='sequence'))\\n\"\n",
        "\n",
        "with open('config.py', 'w') as file:\n",
        "  file.writelines(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIealehOJV9f"
      },
      "outputs": [],
      "source": [
        "# Run trial again: sequence parallelism (2) x pipeline parallelism (2)\n",
        "!colossalai run --nproc_per_node 4 train.py -s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQu7X-YYKCZV",
        "outputId": "0cd1fa30-c733-460b-8c9c-9332735bd59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# make sure to run before executing subsequent codes\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcrFCW05JiUF"
      },
      "source": [
        "### Auto Parallelism\n",
        "\n",
        "Configuring parallism is made easier via auto-parallelism! Try out this experimental feature and watch out for its active development updates!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jom6BN1tJljW",
        "outputId": "9f88c98e-7ba6-44b2-fb36-523c931ab493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/auto_parallel\n"
          ]
        }
      ],
      "source": [
        "%cd ./tutorial/auto_parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C53UQ-NKZEt",
        "outputId": "ef125d88-e881-458e-a2a2-d4cdb2fc3537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auto_ckpt_batchsize_test.py   bench_utils.py  requirements.txt\n",
            "auto_ckpt_solver_test.py      config.py       setup.py\n",
            "auto_parallel_with_resnet.py  README.md       test_ci.sh\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsxGWBJ7KZ_e",
        "outputId": "54c24dc7-9ee4-42b7-8a70-e8cf7a270fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import torch\n",
            "from torchvision.models import resnet50\n",
            "from tqdm import tqdm\n",
            "\n",
            "import colossalai\n",
            "from colossalai.auto_parallel.tensor_shard.initialize import initialize_model\n",
            "from colossalai.core import global_context as gpc\n",
            "from colossalai.device.device_mesh import DeviceMesh\n",
            "from colossalai.logging import get_dist_logger\n",
            "from colossalai.nn.lr_scheduler import CosineAnnealingLR\n",
            "\n",
            "\n",
            "def synthesize_data():\n",
            "    img = torch.rand(gpc.config.BATCH_SIZE, 3, 32, 32)\n",
            "    label = torch.randint(low=0, high=10, size=(gpc.config.BATCH_SIZE,))\n",
            "    return img, label\n",
            "\n",
            "\n",
            "def main():\n",
            "    colossalai.launch_from_torch(config='./config.py')\n",
            "\n",
            "    logger = get_dist_logger()\n",
            "\n",
            "    # trace the model with meta data\n",
            "    model = resnet50(num_classes=10).cuda()\n",
            "\n",
            "    input_sample = {'x': torch.rand([gpc.config.BATCH_SIZE * torch.distributed.get_world_size(), 3, 32, 32]).to('meta')}\n",
            "    device_mesh = DeviceMesh(physical_mesh_id=torch.tensor([0, 1, 2, 3]), mesh_shape=[2, 2], init_process_group=True)\n",
            "    model, solution = initialize_model(model, input_sample, device_mesh=device_mesh, return_solution=True)\n",
            "\n",
            "    if gpc.get_global_rank() == 0:\n",
            "        for node_strategy in solution:\n",
            "            print(node_strategy)\n",
            "    # build criterion\n",
            "    criterion = torch.nn.CrossEntropyLoss()\n",
            "\n",
            "    # optimizer\n",
            "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
            "\n",
            "    # lr_scheduler\n",
            "    lr_scheduler = CosineAnnealingLR(optimizer, total_steps=gpc.config.NUM_EPOCHS)\n",
            "\n",
            "    for epoch in range(gpc.config.NUM_EPOCHS):\n",
            "        model.train()\n",
            "\n",
            "        # if we use synthetic data\n",
            "        # we assume it only has 10 steps per epoch\n",
            "        num_steps = range(10)\n",
            "        progress = tqdm(num_steps)\n",
            "\n",
            "        for _ in progress:\n",
            "            # generate fake data\n",
            "            img, label = synthesize_data()\n",
            "\n",
            "            img = img.cuda()\n",
            "            label = label.cuda()\n",
            "            optimizer.zero_grad()\n",
            "            output = model(img)\n",
            "            train_loss = criterion(output, label)\n",
            "            train_loss.backward(train_loss)\n",
            "            torch.cuda.synchronize()\n",
            "            optimizer.step()\n",
            "        lr_scheduler.step()\n",
            "\n",
            "        # run evaluation\n",
            "        model.eval()\n",
            "        correct = 0\n",
            "        total = 0\n",
            "\n",
            "        # if we use synthetic data\n",
            "        # we assume it only has 10 steps for evaluation\n",
            "        num_steps = range(10)\n",
            "        progress = tqdm(num_steps)\n",
            "\n",
            "        for _ in progress:\n",
            "            # generate fake data\n",
            "            img, label = synthesize_data()\n",
            "\n",
            "            img = img.cuda()\n",
            "            label = label.cuda()\n",
            "\n",
            "            with torch.no_grad():\n",
            "                output = model(img)\n",
            "                test_loss = criterion(output, label)\n",
            "            pred = torch.argmax(output, dim=-1)\n",
            "            correct += torch.sum(pred == label)\n",
            "            total += img.size(0)\n",
            "\n",
            "        logger.info(\n",
            "            f\"Epoch {epoch} - train loss: {train_loss:.5}, test loss: {test_loss:.5}, acc: {correct / total:.5}, lr: {lr_scheduler.get_last_lr()[0]:.5g}\",\n",
            "            ranks=[0])\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n"
          ]
        }
      ],
      "source": [
        "!cat auto_parallel_with_resnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z31R-2NFKrI1"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "pip install -r requirements.txt\n",
        "conda install -c conda-forge coin-or-cbc  # dependency for strategy search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyAp2NCzLMdX"
      },
      "outputs": [],
      "source": [
        "# Execeute the program. It takes a short while to search for parallelization strategy.\n",
        "!colossalai run --nproc_per_node 4 auto_parallel_with_resnet.py -s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgxiZAcTMQtv",
        "outputId": "76ba6ded-f7e8-4e19-bfa0-87299723b5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Return to parent directory\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5O_H2zMjU3"
      },
      "source": [
        "### OPT Training Inference\n",
        "\n",
        "There are sections for [OPT](https://arxiv.org/abs/2205.01068) finetuning and inference. This serves as a good demonstration for using ColossalAI-native [parameters](https://colossalai.org/docs/basics/colotensor_concept), [ZeRo](https://colossalai.org/docs/features/zero_with_chunk) optimizer, and [Gemini](https://colossalai.org/docs/advanced_tutorials/meet_gemini) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_nFXM6QMn_x",
        "outputId": "4b2b1de8-4597-4e8f-b5b9-00a92853b354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/opt/opt\n"
          ]
        }
      ],
      "source": [
        "# Let's first try opt fine-tuning\n",
        "%cd ./tutorial/opt/opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHP5uGz6OrHy",
        "outputId": "ad8e3e0c-3b3a-4dba-f3bb-abddd0e23a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "benchmark.sh\t    context.py\trequirements.txt  run_clm.sh\n",
            "colossalai_zero.py  README.md\trun_clm.py\t  run_clm_synthetic.sh\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ejw-R6tOr34"
      },
      "outputs": [],
      "source": [
        "!cat run_clm.py  # inspect training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhkBjebpOiMw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_phS8YYO-c8"
      },
      "outputs": [],
      "source": [
        "# Run on one GPU; when cuda memory is insufficient, tensors will be offloaded to CPU \n",
        "!bash run_clm_synthetic.sh "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVek837YPFGB"
      },
      "outputs": [],
      "source": [
        "# Train on 4 GPUs\n",
        "!bash run_clm_synthetic.sh 16 0 125m 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeOMnjq6Pj-Z",
        "outputId": "9c6e1414-ae56-4bab-8dd2-a8dbeda8c06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ColossalAI/examples/tutorial/opt/inference\n"
          ]
        }
      ],
      "source": [
        "# Now let's try inference on OPT, powered by our Energeon inference framework \n",
        "# (https://github.com/hpcaitech/EnergonAI/). \n",
        "%cd ../inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJPXi4OlQnOQ"
      },
      "outputs": [],
      "source": [
        "!cat README.md  # refer below for instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S64qElKBP0h8"
      },
      "outputs": [],
      "source": [
        "# Best to run in a separate terminal \n",
        "%%sh \n",
        "docker pull hpcaitech/tutorial:opt-inference\n",
        "docker run -it --rm --gpus all --ipc host -p 7070:7070 hpcaitech/tutorial:opt-inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi0HILIKQRpn"
      },
      "outputs": [],
      "source": [
        "# Run an example trial (best to run in a separate terminal as above)\n",
        "!python opt_fastapi.py opt-125m --tp 2 --checkpoint /data/opt-125m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_3sYyccQ3hm"
      },
      "outputs": [],
      "source": [
        "%cd ../../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fastfold Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'FastFold'...\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "git clone https://github.com/hpcaitech/FastFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ./FastFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up environment\n",
        "%%sh\n",
        "# Run the below commands in your terminal, restart the notebook with kernel named python(fastfold) \n",
        "# conda deactivate\n",
        "# conda env create --name=fastfold -f environment.yml\n",
        "python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download datasets\n",
        "!bash ./scripts/download_all_data.sh data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference\n",
        "!bash inference.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More details on fastfold (including faster kernel operations for training and data processing) can be found [here](https://github.com/hpcaitech/FastFold/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stable Diffusion Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ./tutorial/stable_diffusion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up environment\n",
        "%%sh\n",
        "# Run the below commands in your terminal, restart the notebook with kernel named python(ldm) \n",
        "# conda deactivate\n",
        "# conda env create -f environment.yaml\n",
        "\n",
        "# Install additional dependencies\n",
        "pip install transformers==4.19.2 diffusers invisible-watermark\n",
        "pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the model checkpoint from stable-diffusion-v2-base\n",
        "%%sh\n",
        "wget https://huggingface.co/stabilityai/stable-diffusion-2-base/resolve/main/512-base-ema.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See example usage of inference utility\n",
        "%%sh\n",
        "python scripts/txt2img.py --help "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try stable diffusion model inference \n",
        "%%sh\n",
        "python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms\n",
        "    --outdir ./output \\\n",
        "    --ckpt 512-base-ema.ckpt \\\n",
        "    --config configs/inference/v2-inference.yaml  \\  # there are more inference configs in the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajr2QLqCQ6ge"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We are now finished with our tutorials. Keep updated on our [repository](https://colossalai.org/) for exciting new developments!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "933351abc630374ba7d69d29305d8827306c5eb75384e02fe79e055db4d99722"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
